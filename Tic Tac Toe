import random
from collections import defaultdict

EMPTY = 0
X = 1   # Learning agent
O = 2   # Random opponent

def new_board():
    return [EMPTY]*9

def moves(b):
    return [i for i,v in enumerate(b) if v==EMPTY]

def win(b):
    w = [(0,1,2),(3,4,5),(6,7,8),
         (0,3,6),(1,4,7),(2,5,8),
         (0,4,8),(2,4,6)]
    for a,b2,c in w:
        if b[a]==b[b2]==b[c]!=EMPTY: return b[a]
    if EMPTY not in b: return 0
    return None

def key(b): return tuple(b)

Q = defaultdict(float)
alpha, gamma, eps = 0.5, 0.9, 0.2

def choose(b):
    if random.random() < eps:
        return random.choice(moves(b))
    vals = [(Q[(key(b),a)], a) for a in moves(b)]
    return max(vals)[1]

def train(episodes=5000):
    global eps
    for _ in range(episodes):
        b = new_board()
        hist = []
        p = X
        while True:
            a = choose(b) if p==X else random.choice(moves(b))
            b2 = b.copy(); b2[a] = p
            hist.append((b,a,b2))
            r = win(b2)
            if r is not None:
                reward = 1 if r==X else -1 if r==O else 0
                for s,a1,s2 in reversed(hist):
                    Q[(key(s),a1)] += alpha * (reward - Q[(key(s),a1)])
                    reward *= 0.8
                break
            else:
                if p==X:
                    Q[(key(b),a)] += alpha * (0 + gamma*max(Q[(key(b2),m)] for m in moves(b2)) - Q[(key(b),a)])
            b = b2
            p = O if p==X else X
        eps = max(0.01, eps*0.999)

def play():
    b = new_board()
    p = X
    while True:
        if p==X:
            a = choose(b)
            print("AI moves:", a)
        else:
            print(b[:3], "\n", b[3:6], "\n", b[6:])
            a = int(input("Your move (0â€“8): "))
        b[a] = p
        r = win(b)
        if r is not None:
            print(b[:3], "\n", b[3:6], "\n", b[6:])
            print("Result:", "AI wins" if r==X else "You win" if r==O else "Draw")
            return
        p = O if p==X else X

train(3000)
play()
